<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>VTDexManip: Visual-Tactile Pretraining for Dexterous Manipulation</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
      line-height: 1.6;
    }
    .container {
      max-width: 960px;
      margin: 40px auto;
      padding: 0 20px;
    }
    .title {
      font-size: 36px;
      font-weight: 700;
      text-align: center;
      margin-bottom: 20px;
    }
    .authors {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 15px;
      margin-bottom: 10px;
    }
    .author {
      font-size: 18px;
    }
    .affiliation {
      text-align: center;
      font-size: 18px;
      color: #555;
      margin-bottom: 20px;
    }
    .links {
      text-align: center;
      margin-bottom: 30px;
    }
    .links a {
      font-size: 18px;
      margin: 0 10px;
      text-decoration: none;
      color: #0066cc;
      padding: 8px 14px;
      border: 1px solid #ccc;
      border-radius: 6px;
      transition: background-color 0.3s;
    }
    .links a:hover {
      background-color: #e0f0ff;
    }
    h2 {
      font-size: 24px;
      margin-top: 40px;
      border-bottom: 2px solid #ccc;
      padding-bottom: 6px;
    }
    p {
      font-size: 16px;
      text-align: justify;
    }
    iframe {
      display: block;
      margin: 20px auto;
      border: none;
      border-radius: 10px;
    }
    .video-placeholder {
      width: 100%;
      height: 400px;
      background-color: #ddd;
      display: flex;
      align-items: center;
      justify-content: center;
      color: #555;
      font-size: 20px;
      border-radius: 12px;
      margin: 20px 0;
    }
    .footer {
      text-align: center;
      margin: 50px 0 20px;
      font-size: 14px;
      color: #777;
    }
    .footer a {
      color: #555;
    }
  </style>
</head>
<body>

<div class="container">

  <div class="title">VTDexManip: A Dataset and Benchmark for Visual-Tactile Pretraining and Dexterous Manipulation With Reinforcement Learning</div>

  <div class="authors">
    <div class="author"><a href="https://LQTS.github.io/">Qingtao Liu<sup>1</sup></a></div>
    <div class="author">Yu Cui<sup>1</sup></div>
    <div class="author">Zhengnan Sun<sup>1</sup></div>
    <div class="author"><a href="https://person.zju.edu.cn/en/gaofengli">Gaofeng Li<sup>1</sup></a></div>
    <div class="author"><a href="https://person.zju.edu.cn/jmchen">Jiming Chen<sup>1</sup></a></div>
    <div class="author"><a href="https://person.zju.edu.cn/yeqi">Qi Ye<sup>1✝</sup></a></div>
  </div>

  <div class="affiliation"><sup>1</sup>Zhejiang University</div>

  <div class="links">
    <a href="https://openreview.net/pdf?id=jf7C7EGw21">[Paper]</a>
    <a href="https://github.com/LQTS/VTDexManip">[Code & Dataset]</a>
    <a href="https://www.bilibili.com/video/BV15AfnYBEB1">[Project Video]</a>
  </div>

  <h2>Abstract</h2>
  <p>
    Vision and touch are essential sensory modalities in human manipulation. While prior works have leveraged human manipulation videos for pretraining robotic skills, most are limited to vision-language modalities and simple grippers.
    To address this, we collect a large-scale visual-tactile dataset of humans performing 10 daily manipulation tasks across 182 objects. Our dataset is the first to support complex dexterous manipulation with visual-tactile information.
  </p>
  <p>
    We introduce a benchmark of 6 dexterous tasks and develop a reinforcement learning-based visual-tactile policy learning framework. We compare 17 baseline and pretraining strategies to assess the impact of different modalities.
  </p>
  <p>
    Key results show: (1) incorporating sparse binary tactile signals boosts policy performance by ~20%, (2) joint visual-tactile pretraining improves generalization across tasks, and (3) the visual-tactile policy is robust to sensor noise and viewpoint variation.
  </p>

  <h2>Dataset Statistics</h2>
  <img src="resources/dataset.jpg" alt="Dataset Statistics" style="width:100%; max-width:800px; display:block; margin:20px auto; border-radius:12px;">
  <p style="text-align:left; max-width:800px; margin:10px auto;">
    ★ Overview of our collected visual-tactile dataset, which includes 10 diverse manipulation tasks and 182 objects. (a) Our collection system. (b) The number of trajectories and objects. (c): The number of total frames (On: frames w/ contact; Off: frames w/o contact ). (d) The distribution of the number of frames. (e) t-SNE of the tactile signals
  </p>

  <h2>Benchmark Overview</h2>
  <img src="resources/benchmark1.jpg" alt="Benchmark Overview" style="width:100%; max-width:800px; display:block; margin:20px auto; border-radius:12px;">
  <p style="text-align:left; max-width:800px; margin:10px auto;">
    ★ The proposed benchmark includes 6 complex dexterous manipulation tasks with vision-tactile RL training and evaluation. It is designed to test generalization across tasks and modalities. (a) shows the six tasks of our manipulation platform; (b) lists the 18 pretrained and non-pretrained models in our benchmark; (c) is the policy learning framework combines proprioceptive inputs and perception representations to guide actions within an MDP, with skills learned via PPO.
  </p>

   <h2>Video</h2>
  <!-- &lt;!&ndash;    <iframe src="https://www.bilibili.com/video/BV1bP411b7jh/?spm_id_from=333.999.0.0&vd_source=811a28ecc0ef2e6c74261c8ae4105092" allowfullscreen="allowfullscreen" width="100%" height="500" scrolling="no" frameborder="0" sandbox="allow-top-navigation allow-same-origin allow-forms allow-scripts"></iframe>&ndash;&gt; -->
  <!-- &lt;!&ndash; <iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://www.bilibili.com/video/BV1bP411b7jh/?spm_id_from=333.999.0.0&vd_source=811a28ecc0ef2e6c74261c8ae4105092&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="false"> </iframe>&ndash;&gt; -->
  <iframe src="https://player.bilibili.com/player.html?aid=113881309514469&bvid=BV1bP411b7jh&cid=28047706206&p=1" allowfullscreen="allowfullscreen" width="80%" height="500" scrolling="no" frameborder="0" sandbox="allow-top-navigation allow-same-origin allow-forms allow-scripts"></iframe>
  <!-- &lt;!&ndash;<iframe src="//player.bilibili.com/player.html?aid=318689685&bvid=BV1bP411b7jh&cid=1271612680&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>&ndash;&gt; -->


  <h2> BibTeX</h2>
  <table align=center width=1000px height=250px>
  <tr>
    <td align=left width=200px><a href=""><img class="layered-paper-big" style="height:180px" src="./resources/paper_preview.png"/></a></td>
    <td width=700px style="color: #4d4b59; font-size:12pt">
      <code>
        @inproceedings{liu2025vtdexmanip, <br>
        title={VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning},<br>
        author={Qingtao Liu and Yu Cui and Zhengnan Sun and Gaofeng Li and Jiming Chen and Qi Ye},<br>
        booktitle={The Thirteenth International Conference on Learning Representations},<br>
        year={2025},<br>
        url={https://openreview.net/forum?id=jf7C7EGw21}<br>
        }
      </code>
    </td>
  </tr>
  </table>


  <h2>Contact</h2>
  <p>For questions and collaborations, contact:</p>
  <p><a href="mailto:l_qingtao@zju.edu.cn">l_qingtao@zju.edu.cn</a>, <a href="mailto:qi.ye@zju.edu.cn">qi.ye@zju.edu.cn</a></p>

</div>

<div class="footer">
  &copy; 2025 VTDexManip. Maintained by the authors.
</div>

</body>
</html>
